{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.python.keras.layers import  Convolution2D, MaxPooling2D\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import applications\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpiar sesiones de keras, para tener la session limpia\n",
    "K.clear_session()\n",
    "\n",
    "#cargo datos de entrenamiento y valdidacion\n",
    "data_entrenamiento = './data/entrenamiento'\n",
    "data_validacion = './data/validacion'\n",
    "\n",
    "#Parametros\n",
    "\n",
    "#Numero de veces que iteramos en todo el set de datos\n",
    "epocas=20\n",
    "\n",
    "#Tamaño de las imagenes estandar\n",
    "longitud, altura = 150, 150\n",
    "\n",
    "#Numero de imagenes que se envian en cada paso\n",
    "batch_size = 32\n",
    "\n",
    "#Numero de veces que se procesa la infomacion en cada Epoca\n",
    "pasos = 1000\n",
    "\n",
    "#Al final de cada Epoca se corren 300 pasos de validacion\n",
    "validation_steps = 300\n",
    "\n",
    "#Tamaño despues de cada convulucion\n",
    "filtrosConv1 = 32\n",
    "filtrosConv2 = 64\n",
    "\n",
    "\n",
    "tamano_filtro1 = (3, 3)\n",
    "tamano_filtro2 = (2, 2)\n",
    "tamano_pool = (2, 2)\n",
    "clases = 3\n",
    "\n",
    "#Learning rate\n",
    "lr = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 images belonging to 3 classes.\n",
      "Found 2043 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "##Pre procesamiento nuestras imagenes\n",
    "entrenamiento_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,  #Rescala las imagenes los pixels van a ir a valores de 0-1\n",
    "    shear_range=0.2, #inclina imagenes\n",
    "    zoom_range=0.2, #algunas fotos se les hace zoom a otroas no\n",
    "    horizontal_flip=True) #invierte imagenes  para aprender direccion\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "#preprocesa la carpeta de data\n",
    "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "#pre procesa la carpeta de validacion\n",
    "validacion_generador = test_datagen.flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red neuronal sequencial, varias capas aplicadas\n",
    "cnn = Sequential()\n",
    "#Primera capa de red neuronal\n",
    "cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(longitud, altura, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "#Segunda capa de red neuronal\n",
    "cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\"))\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(clases, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "64/64 [==============================] - 22s 337ms/step - loss: 0.7812 - acc: 0.5046\n",
      "32/32 [==============================] - 58s 2s/step - loss: 1.4414 - acc: 0.5275 - val_loss: 0.7812 - val_acc: 0.5046\n",
      "Epoch 2/20\n",
      "64/64 [==============================] - 21s 336ms/step - loss: 0.7039 - acc: 0.6055\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.8070 - acc: 0.5596 - val_loss: 0.7039 - val_acc: 0.6055\n",
      "Epoch 3/20\n",
      "64/64 [==============================] - 22s 344ms/step - loss: 0.7520 - acc: 0.5370\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7446 - acc: 0.5946 - val_loss: 0.7520 - val_acc: 0.5370\n",
      "Epoch 4/20\n",
      "64/64 [==============================] - 22s 340ms/step - loss: 0.7453 - acc: 0.5423\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7179 - acc: 0.6006 - val_loss: 0.7453 - val_acc: 0.5423\n",
      "Epoch 5/20\n",
      "64/64 [==============================] - 22s 343ms/step - loss: 0.7250 - acc: 0.6021\n",
      "32/32 [==============================] - 59s 2s/step - loss: 0.7107 - acc: 0.6216 - val_loss: 0.7250 - val_acc: 0.6021\n",
      "Epoch 6/20\n",
      "64/64 [==============================] - 22s 339ms/step - loss: 0.7225 - acc: 0.5825\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7075 - acc: 0.6206 - val_loss: 0.7225 - val_acc: 0.5825\n",
      "Epoch 7/20\n",
      "64/64 [==============================] - 22s 337ms/step - loss: 0.6837 - acc: 0.6187\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.6780 - acc: 0.6266 - val_loss: 0.6837 - val_acc: 0.6187\n",
      "Epoch 8/20\n",
      "64/64 [==============================] - 19s 305ms/step - loss: 0.6607 - acc: 0.6437\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.6766 - acc: 0.6567 - val_loss: 0.6607 - val_acc: 0.6437\n",
      "Epoch 9/20\n",
      "64/64 [==============================] - 20s 319ms/step - loss: 0.7791 - acc: 0.5884\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6495 - acc: 0.6677 - val_loss: 0.7791 - val_acc: 0.5884\n",
      "Epoch 10/20\n",
      "64/64 [==============================] - 21s 332ms/step - loss: 0.6430 - acc: 0.6701\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.6366 - acc: 0.6657 - val_loss: 0.6430 - val_acc: 0.6701\n",
      "Epoch 11/20\n",
      "64/64 [==============================] - 16s 249ms/step - loss: 0.6821 - acc: 0.6270\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.6108 - acc: 0.6777 - val_loss: 0.6821 - val_acc: 0.6270\n",
      "Epoch 12/20\n",
      "64/64 [==============================] - 16s 256ms/step - loss: 0.7074 - acc: 0.6417\n",
      "32/32 [==============================] - 44s 1s/step - loss: 0.6273 - acc: 0.6897 - val_loss: 0.7074 - val_acc: 0.6417\n",
      "Epoch 13/20\n",
      "64/64 [==============================] - 17s 264ms/step - loss: 0.6269 - acc: 0.6721\n",
      "32/32 [==============================] - 45s 1s/step - loss: 0.6067 - acc: 0.6907 - val_loss: 0.6269 - val_acc: 0.6721\n",
      "Epoch 14/20\n",
      "64/64 [==============================] - 18s 282ms/step - loss: 0.6403 - acc: 0.6735\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.6060 - acc: 0.6877 - val_loss: 0.6403 - val_acc: 0.6735\n",
      "Epoch 15/20\n",
      "64/64 [==============================] - 19s 301ms/step - loss: 0.6353 - acc: 0.6730\n",
      "32/32 [==============================] - 48s 2s/step - loss: 0.5923 - acc: 0.7087 - val_loss: 0.6353 - val_acc: 0.6730\n",
      "Epoch 16/20\n",
      "64/64 [==============================] - 18s 284ms/step - loss: 0.6189 - acc: 0.6838\n",
      "32/32 [==============================] - 47s 1s/step - loss: 0.5432 - acc: 0.7247 - val_loss: 0.6189 - val_acc: 0.6838\n",
      "Epoch 17/20\n",
      "64/64 [==============================] - 18s 276ms/step - loss: 0.6591 - acc: 0.6647\n",
      "32/32 [==============================] - 49s 2s/step - loss: 0.5340 - acc: 0.7417 - val_loss: 0.6591 - val_acc: 0.6647\n",
      "Epoch 18/20\n",
      "64/64 [==============================] - 19s 294ms/step - loss: 0.6749 - acc: 0.6667\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.5129 - acc: 0.7497 - val_loss: 0.6749 - val_acc: 0.6667\n",
      "Epoch 19/20\n",
      "64/64 [==============================] - 18s 277ms/step - loss: 0.6381 - acc: 0.6985\n",
      "32/32 [==============================] - 46s 1s/step - loss: 0.4988 - acc: 0.7558 - val_loss: 0.6381 - val_acc: 0.6985\n",
      "Epoch 20/20\n",
      "64/64 [==============================] - 18s 277ms/step - loss: 0.6579 - acc: 0.6892\n",
      "32/32 [==============================] - 47s 1s/step - loss: 0.5115 - acc: 0.7538 - val_loss: 0.6579 - val_acc: 0.6892\n"
     ]
    }
   ],
   "source": [
    "#Optimizar modelo de Redes neuronales\n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=optimizers.Adam(lr=lr),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "#Entrenar nuestra red Neuronal con las imagenes de entrenamiento\n",
    "cnn.fit_generator(\n",
    "    entrenamiento_generador,\n",
    "    steps_per_epoch=pasos,\n",
    "    epochs=epocas,\n",
    "    validation_data=validacion_generador,\n",
    "    validation_steps=validation_steps)\n",
    "\n",
    "target_dir = './modelo/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "cnn.save('./modelo/modelo.h5')\n",
    "cnn.save_weights('./modelo/pesos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "longitud, altura = 150, 150\n",
    "modelo = './modelo/modelo.h5'\n",
    "pesos_modelo = './modelo/pesos.h5'\n",
    "cnn = load_model(modelo)\n",
    "cnn.load_weights(pesos_modelo)\n",
    "\n",
    "def predict(file):\n",
    "    x = load_img(file, target_size=(longitud, altura))\n",
    "    x = img_to_array(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    array = cnn.predict(x)\n",
    "    result = array[0]\n",
    "    answer = np.argmax(result)\n",
    "    if answer == 0:\n",
    "        print(\"pred: Perro\")\n",
    "    elif answer == 1:\n",
    "        print(\"pred: Gato\")\n",
    "    elif answer == 2:\n",
    "        print(\"pred: Gorila\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Perro\n"
     ]
    }
   ],
   "source": [
    "predict('dog.4006.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
